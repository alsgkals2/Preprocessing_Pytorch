{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2400f752-2585-4a5e-87c5-e426e00d17b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaError: Run 'conda init' before 'conda activate'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !conda init\n",
    "# !exec $SHELL\n",
    "!conda activate clipenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149ce139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "import random\n",
    "import Logger\n",
    "import torch\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "# from test import test\n",
    "import mobileclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16aafe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--pretrained_path',type=str)\n",
    "    parser.add_argument('--model', default='mobileclip_s0', type=str)\n",
    "    parser.add_argument('--model_cfg', default='mobileclip_s0_for_binary', type=str)\n",
    "    parser.add_argument('--input_size', default=128, type=int)\n",
    "    parser.add_argument('--test', action='store_true')\n",
    "    return parser\n",
    "parser = get_args_parser()\n",
    "args = parser.parse_args(args=[])\n",
    "log = Logger.Logger()\n",
    "from datetime import datetime\n",
    "title = datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "if not args.test:\n",
    "    args.pretrained_path = os.path.join(args.pretrained_path, args.model) + '.pt'\n",
    "print('args.pretrained_path:', args.pretrained_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4d1db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.pretrained_path='./mobileclip_mh/weights/checkpoint.pth'\n",
    "model, _, preprocess = mobileclip.create_model_and_transforms(args, args.model, pretrained=args.pretrained_path, reparameterize=False)\n",
    "tokenizer = mobileclip.get_tokenizer(args.model_cfg)\n",
    "label_to_caption = [\n",
    "                    \"This is an example of a real face\",\n",
    "                    \"This is a bonafide face\",\n",
    "                    \"This is a real face\",\n",
    "                    \"This is how a real face looks like\",\n",
    "                    \"a photo of a real face\",\n",
    "                    \"This is not a spoof face\",\n",
    "                    \"This is an example of a spoof face\",\n",
    "                    \"This is an example of an attack face\",\n",
    "                    \"This is not a real face\",\n",
    "                    \"This is how a spoof face looks like\",\n",
    "                    \"a photo of a spoof face\",\n",
    "                    \"a printout shown to be a spoof face\",\n",
    "                    ]\n",
    "# images in skimage to use and their textual descriptions\n",
    "descriptions = {\n",
    "    \"astronaut\": \"a portrait of an astronaut with the American flag\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c61c6ba-4e89-4130-b4ca-e2f6a395d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import skimage\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# image = self.preprocess(Image.open(image_path).convert('RGB'))\n",
    "# label = self.labels[idx]\n",
    "original_images = []\n",
    "images = []\n",
    "texts = []\n",
    "for filename in [filename for filename in os.listdir(skimage.data_dir) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]:\n",
    "    name = os.path.splitext(filename)[0]\n",
    "    if name not in descriptions:\n",
    "        continue\n",
    "    image = Image.open(os.path.join(skimage.data_dir, filename)).convert(\"RGB\")\n",
    "    original_images.append(image)\n",
    "    images.append(preprocess(image))\n",
    "    caption = random.choice(label_to_caption)\n",
    "    text = tokenizer(caption, return_tensors=\"pt\").squeeze()\n",
    "    texts.append(text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f46733e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "image_features = model.image_encoder(images[0].unsqueeze(0).cuda())\n",
    "text_features = model.text_encoder(texts[0].unsqueeze(0).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce81da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = './tflite_tmp'\n",
    "os.makedirs(output, exist_ok=True)\n",
    "torch.onnx.export(model.text_encoder, texts[0].unsqueeze(0).cuda(), os.path.join(output,\"mobileclip-text-vit-32_v2.onnx\"), verbose=False, opset_version=12, input_names=['images'],\n",
    "                    output_names=['output'],\n",
    "                    dynamic_axes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "349c157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# torch.onnx.export(model.image_encoder, images[0].unsqueeze(0).cuda(), os.path.join(output,\"mobileclip-image-vit-32.onnx\"), export_params=True, opset_version=14, do_constant_folding=True, input_names = ['input'], output_names = ['output'], dynamic_axes={'input' : {0 : 'batch_size'}, 'output' : {0 : 'batch_size'}})\n",
    "# torch.onnx.export(model.image_encoder, images[0].unsqueeze(0).cuda(), os.path.join(output,\"mobileclip-image-vit-32.onnx\"), export_params=True, opset_version=14, input_names = ['input'], output_names = ['output'])\n",
    "torch.onnx.export(model.image_encoder, images[0].unsqueeze(0).cuda(), os.path.join(output,\"mobileclip-image-vit-32_v2.onnx\"), verbose=False, opset_version=12, input_names=['images'],\n",
    "                    output_names=['output'],\n",
    "                    dynamic_axes=None)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "250fd11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt at quantizing model to uint8 (doesn't seem to work? no errors, but onnx file is same size)\n",
    "# Reference: https://github.com/minimaxir/imgbeddings/blob/36fb4d7ac6b82694d109cef6f887d4cb9c49da0f/imgbeddings/models.py#L94\n",
    "# Here's the model the above code generates: https://huggingface.co/minimaxir/imgbeddings/blob/main/patch32_v1.onnx\n",
    "# Here's a demo of the above ONNX model with ORT Web: https://jsbin.com/nupehazaju/edit?html,output  <-- seems to work, but this model doesn't have the projection head that squashes 768 vec to 512 elements (so can be compared to text embeddings of same length)\n",
    "# !pip install onnxruntime\n",
    "# !pip install onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd5ccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# quantize_dynamic(os.path.join(output,\"mobileclip-image-vit-32.onnx\"), os.path.join(output,\"mogileclip-image-vit-32-uint8.onnx\"), weight_type=QuantType.QUInt8, extra_options={\"MatMulConstBOnly\":False}) # I added the MatMulConstBOnly as a guess due to warnings that it outputs without it\n",
    "# quantize_dynamic(os.path.join(output,\"mobileclip-text-vit-32.onnx\"), os.path.join(output,\"mogileclip-text-vit-32-uint8.onnx\"), weight_type=QuantType.QUInt8, extra_options={\"MatMulConstBOnly\":False}) # I added the MatMulConstBOnly as a guess due to warnings that it outputs without it\n",
    "quantize_dynamic(os.path.join(output,\"mobileclip-text-vit-32.onnx\"), os.path.join(output,\"mogileclip-text-vit-32-uint8_int8.onnx\"), weight_type=QuantType.QInt8, extra_options={\"MatMulConstBOnly\":False}) # I added the MatMulConstBOnly as a guess due to warnings that it outputs without it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796ced41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/onnx/onnx-tensorflow.git\n",
    "#export tflite using onnx\n",
    "!onnx-tf convert -i tflite_tmp/mobileclip-text-vit-32_v2.onnx -o tflite_tmp/mobileclip-text-vit-32-tf_v2\n",
    "!onnx-tf convert -i tflite_tmp/mobileclip-image-vit-32_v2.onnx -o tflite_tmp/mobileclip-image-vit-32-tf_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dfb10c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflowjs\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bf1150",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# image encoder:\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(output,\"mobileclip-image-vit-32-tf_v2\"))\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS] # This line is needed because: https://github.com/tensorflow/tfjs/issues/5844\n",
    "tflite_model = converter.convert()\n",
    "with open(os.path.join(output,'mobileclip-image-vit-32_v2.tflite'), 'wb') as f:\n",
    "  f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7532f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text encoder:\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(output,\"mobileclip-text-vit-32-tf_v2_float\"))\n",
    "# converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(output,\"mobileclip-text-vit-32-tf_v2\"))\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS] # This line is needed because: https://github.com/tensorflow/tfjs/issues/5844\n",
    "tflite_model = converter.convert()\n",
    "with open(os.path.join(output,'mobileclip-text-vit-32-tf_v2_float.tflite'), 'wb') as f:\n",
    "  f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ce311f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
