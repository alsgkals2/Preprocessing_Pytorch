{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb21ba60-3032-4183-bef6-24e23c1af631",
   "metadata": {},
   "source": [
    "### CUT AND SAVE IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91174e8a-7349-48f5-9a1e-205da4f45f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torchvision \n",
    "from torchvision import transforms, datasets\n",
    "import torchvision.datasets as dataset\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import time\n",
    "import glob\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00bd96e-4253-47b0-9929-5b65f2a6cdfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d4ed343e-f7be-4f28-a5de-8db10384a4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "200\n",
      "tensor([[0.9430, 0.6997, 0.5442,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.9135, 0.8915, 0.7127,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.9264, 0.9913, 0.9893,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t = torch.tensor(np.random.random_sample((3,200,100)))\n",
    "# functional.crop(t,100,200,200,200)\n",
    "based_t = torch.zeros((3,500,300))\n",
    "based_t[:,:200,:100] = t\n",
    "print(based_t.size(1))\n",
    "print(t.size(1))\n",
    "print(based_t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcbeb204-09a2-4692-aeef-2af816248b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms\n",
    "from torchvision.transforms import functional\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "func_crop = functional.crop\n",
    "\n",
    "def cut_img_png(img_path_list, save_path, stride=128):\n",
    "    img_size = 256\n",
    "    toimage = transforms.ToPILImage()\n",
    "    totensor = transforms.ToTensor()\n",
    "    os.makedirs(f'{save_path}{img_size}', exist_ok=True)\n",
    "    num = 0\n",
    "    for path in tqdm(img_path_list):\n",
    "        img = Image.open(path)\n",
    "        img = totensor(img)\n",
    "        for top in range(0, img.size()[2], stride):\n",
    "            for left in range(0, img.size()[1], stride):\n",
    "                based_tensor = torch.zeros((3,img_size,img_size))\n",
    "                arr_cropped = func_crop(img,top,left,img_size,img_size)\n",
    "                w, h = arr_cropped.size(1), arr_cropped.size(2)\n",
    "                based_tensor[:, :w, :h] = arr_cropped\n",
    "                to_pil = toimage(based_tensor)\n",
    "                to_pil.save(f'{save_path}{img_size}/{num}.png')\n",
    "                num+=1\n",
    "                \n",
    "def cut_img(img_path_list, save_path, stride):\n",
    "    img_size = 256\n",
    "    os.makedirs(f'{save_path}{img_size}', exist_ok=True)\n",
    "    num = 0\n",
    "    for path in tqdm(img_path_list):\n",
    "        img = cv2.imread(path)\n",
    "        for top in range(0, img.shape[0], stride):\n",
    "            for left in range(0, img.shape[1], stride):\n",
    "                piece = np.zeros([img_size, img_size, 3], np.uint8)\n",
    "                temp = img[top:top+img_size, left:left+img_size, :]\n",
    "                piece[:temp.shape[0], :temp.shape[1], :] = temp\n",
    "                np.save(f'{save_path}{img_size}/{num}.npy', piece)\n",
    "                num+=1                \n",
    "\n",
    "def cut_img_half(img_path_list, save_path):\n",
    "    os.makedirs(f'{save_path}half', exist_ok=True)\n",
    "    num = 0\n",
    "    for path in tqdm(img_path_list):\n",
    "        img = cv2.imread(path)\n",
    "        len_half_x = img.shape[1]//2\n",
    "        ratio_len = img.shape[0]/img.shape[1]\n",
    "        len_half_y = img.shape[0]//2\n",
    "        for top in range(0, img.shape[0], len_half_y//2):\n",
    "            for left in range(0, img.shape[1], len_half_x//2):\n",
    "                piece = np.zeros([len_half_y, len_half_x, 3], np.uint8)\n",
    "                temp = img[top:top+len_half_y, left:left+len_half_x, :]\n",
    "                piece[:temp.shape[0], :temp.shape[1], :] = temp\n",
    "                np.save(f'{save_path}half/{num}.npy', piece)\n",
    "                num+=1\n",
    "\n",
    "def cut_img_6devide(img_path_list, save_path):\n",
    "    from tqdm import tqdm\n",
    "    os.makedirs(f'{save_path}6devide', exist_ok=True)\n",
    "    num = 0\n",
    "    for path in tqdm(img_path_list):\n",
    "        img = cv2.imread(path)\n",
    "        len_x = img.shape[1]//6\n",
    "        len_y = img.shape[0]//6\n",
    "        for top in range(0, img.shape[0], len_y//2):\n",
    "            for left in range(0, img.shape[1], len_x//2):\n",
    "                piece = np.zeros([len_y, len_x, 3], np.uint8)\n",
    "                temp = img[top:top+len_y, left:left+len_x, :]\n",
    "                piece[:temp.shape[0], :temp.shape[1], :] = temp\n",
    "                np.save(f'{save_path}6devide/{num}.npy', piece)\n",
    "                num+=1                \n",
    "                \n",
    "\n",
    "def cut_img_fullscreen(img_path_list, save_path):\n",
    "    from tqdm import tqdm\n",
    "    os.makedirs(f'{save_path}full', exist_ok=True)\n",
    "    num = 0\n",
    "    for path in tqdm(img_path_list):\n",
    "        img = cv2.imread(path)\n",
    "        len_x = img.shape[1]\n",
    "        len_y = img.shape[0]\n",
    "        np.save(f'{save_path}full/{num}_full.npy', img)\n",
    "        num+=1                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4617055-14aa-4f0a-ac47-f92cc6999039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 120/120 [25:41<00:00, 12.84s/it]\n",
      "100%|██████████████████████████████| 120/120 [24:05<00:00, 12.04s/it]\n"
     ]
    }
   ],
   "source": [
    "# cut_img_png(train_input_files, './camera_dataset/train_input_img_png_',128)\n",
    "# cut_img_png(train_label_files, './camera_dataset/train_label_img_png_')\n",
    "cut_img_png(val_input_files, './camera_dataset/val_input_img_png_')\n",
    "cut_img_png(val_label_files, './camera_dataset/val_label_img_png_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d6646c8-4240-4bc0-96aa-f786e4391772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_csv = pd.read_csv('./camera_dataset/train.csv')\n",
    "test_csv = pd.read_csv('./camera_dataset/test.csv')\n",
    "train_all_input_files = './camera_dataset/train_input_img/'+train_csv['input_img']\n",
    "train_all_label_files = './camera_dataset/train_label_img/'+train_csv['label_img']\n",
    "train_input_files = train_all_input_files[120:].to_numpy()\n",
    "train_label_files = train_all_label_files[120:].to_numpy()\n",
    "\n",
    "val_input_files = train_all_input_files[:120].to_numpy()\n",
    "val_label_files = train_all_label_files[:120].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68cdee3-266f-4e30-b56b-150dde1d2ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "747ed8e6-177b-4c77-9048-30becfaf241a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./camera_dataset/train_input_img/train_input_10120.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[19, 21, 37],\n",
       "        [19, 21, 37],\n",
       "        [19, 18, 33],\n",
       "        ...,\n",
       "        [ 4, 13,  8],\n",
       "        [ 6, 16, 10],\n",
       "        [ 6, 15, 13]],\n",
       "\n",
       "       [[19, 21, 37],\n",
       "        [19, 21, 37],\n",
       "        [19, 18, 33],\n",
       "        ...,\n",
       "        [ 4, 13,  8],\n",
       "        [ 6, 16, 10],\n",
       "        [ 6, 15, 13]],\n",
       "\n",
       "       [[19, 18, 53],\n",
       "        [19, 18, 53],\n",
       "        [15, 19, 48],\n",
       "        ...,\n",
       "        [ 4, 12, 11],\n",
       "        [ 8, 13, 13],\n",
       "        [ 6, 15, 13]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 6,  7, 14],\n",
       "        [ 6,  7, 14],\n",
       "        [ 8,  6, 14],\n",
       "        ...,\n",
       "        [ 4, 14, 14],\n",
       "        [ 2, 13, 17],\n",
       "        [ 4, 13, 18]],\n",
       "\n",
       "       [[ 6,  7, 11],\n",
       "        [ 6,  7, 11],\n",
       "        [ 6,  8, 11],\n",
       "        ...,\n",
       "        [ 6, 13, 18],\n",
       "        [ 6, 13, 18],\n",
       "        [ 2, 13, 18]],\n",
       "\n",
       "       [[ 4,  7,  8],\n",
       "        [ 4,  7,  8],\n",
       "        [ 2,  9, 10],\n",
       "        ...,\n",
       "        [ 6, 13, 21],\n",
       "        [ 4, 16, 20],\n",
       "        [ 4, 16, 18]]], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(train_input_files[0])\n",
    "\n",
    "np.load('./camera_dataset/train_input_img_256/0.npy')#,allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601aa9b7-141c-4738-814e-cded904268a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_img(train_input_files, './camera_dataset/train_input_img_')\n",
    "cut_img(train_label_files, './camera_dataset/train_label_img_')\n",
    "cut_img(val_input_files, './camera_dataset/val_input_img_')\n",
    "cut_img(val_label_files, './camera_dataset/val_label_img_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91879fd-f931-419b-81e3-341a1050a7df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gogogo",
   "language": "python",
   "name": "mhmh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
