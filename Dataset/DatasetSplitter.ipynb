{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_splitter(DATASET, SPLIT_VALUE, VALIDATION_SIZE, BATCH_SIZE, specific_alpha=False):\n",
    "    dataset_len = len(DATASET) - VALIDATION_SIZE\n",
    "    validation_set, non_alpha_set, alphaset_set = torch.utils.data.random_split(DATASET, [VALIDATION_SIZE,\n",
    "                                                                                          dataset_len - SPLIT_VALUE,\n",
    "                                                                                          SPLIT_VALUE])\n",
    "    NON_ALPHA_DATALOADER = torch.utils.data.DataLoader(non_alpha_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_alpha_dataloader, alpha_dataloader, validation_dataloader, neut_dataloader = dataset_splitter(OBJECT_DATASET,\n",
    "                                                                                                  SPLIT_VALUE,\n",
    "                                                                                                  VALIDATION_SIZE,\n",
    "                                                                                                  batch_size,\n",
    "                                                                                                  specific_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def GetSplitLoaders_TwoClass(list_correct,dataset,train_aug=None,num_store_per=5):\n",
    "    correct_loader=[[],[]]\n",
    "    num_data = 0\n",
    "    for i in range(num_store_per):\n",
    "        list_temp = [list_correct[i][0],list_correct[i][1]]\n",
    "        for rf in range(len(list_temp)):\n",
    "            if not list_temp[rf] :\n",
    "                correct_loader[rf].append([])\n",
    "                continue\n",
    "            temp_dataset = copy.deepcopy(dataset)\n",
    "            temp_dataset.data = np.array(temp_dataset.data[list_correct[i][rf]])\n",
    "            temp_dataset.target = np.array(temp_dataset.target[list_correct[i][rf]])\n",
    "            custum = CustumDataset(temp_dataset.data,temp_dataset.target,train_aug)\n",
    "            correct_loader[rf].append(DataLoader(custum,\n",
    "                                     batch_size=200, shuffle=False, num_workers=4, pin_memory=True))    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
