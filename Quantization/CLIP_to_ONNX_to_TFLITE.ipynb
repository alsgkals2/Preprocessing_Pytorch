{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2400f752-2585-4a5e-87c5-e426e00d17b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/openai/CLIP\n",
    "# !conda activate clipenv\n",
    "# ! pip install ftfy regex tqdm\n",
    "# !sed -i -e 's/def forward(self, image, text):/def old_forward(self, image, text):/g' ./clip/model.py\n",
    "# !sed -i -e 's/def encode_text(self, text):/def forward(self, text):/g' ./clip/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2744dc-5c55-47d9-81e4-32443e11609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import clip\n",
    "\n",
    "clip.available_models()\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "model.cuda().eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length =  model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c61c6ba-4e89-4130-b4ca-e2f6a395d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import skimage\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "\n",
    "# images in skimage to use and their textual descriptions\n",
    "descriptions = {\n",
    "    \"astronaut\": \"a portrait of an astronaut with the American flag\",\n",
    "}\n",
    "\n",
    "original_images = []\n",
    "images = []\n",
    "texts = []\n",
    "for filename in [filename for filename in os.listdir(skimage.data_dir) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]:\n",
    "    name = os.path.splitext(filename)[0]\n",
    "    if name not in descriptions:\n",
    "        continue\n",
    "    image = Image.open(os.path.join(skimage.data_dir, filename)).convert(\"RGB\")\n",
    "    original_images.append(image)\n",
    "    images.append(preprocess(image))\n",
    "    texts.append(descriptions[name])\n",
    "    \n",
    "image_input = torch.tensor(np.stack(images)).cuda()\n",
    "text_tokens = clip.tokenize([\"This is \" + desc for desc in texts]).cuda()\n",
    "\n",
    "model.visual = model.visual.to(torch.float32)\n",
    "model = model.to(torch.float32)\n",
    "model.visual(image_input)[0] # astronaut pic embedding\n",
    "model(text_tokens)[0] # astronaut text embedding\n",
    "torch.onnx.export(model, text_tokens, \"clip-text-vit-32.onnx\", export_params=True, opset_version=14, do_constant_folding=True, input_names = ['input'], output_names = ['output'], dynamic_axes={'input' : {0 : 'batch_size'}, 'output' : {0 : 'batch_size'}})\n",
    "torch.onnx.export(model.visual, image_input, \"clip-image-vit-32.onnx\", export_params=True, opset_version=14, do_constant_folding=True, input_names = ['input'], output_names = ['output'], dynamic_axes={'input' : {0 : 'batch_size'}, 'output' : {0 : 'batch_size'}})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78bb1c9-6f32-49db-96c7-ae4c3a333eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt at quantizing model to uint8 (doesn't seem to work? no errors, but onnx file is same size)\n",
    "# Reference: https://github.com/minimaxir/imgbeddings/blob/36fb4d7ac6b82694d109cef6f887d4cb9c49da0f/imgbeddings/models.py#L94\n",
    "# Here's the model the above code generates: https://huggingface.co/minimaxir/imgbeddings/blob/main/patch32_v1.onnx\n",
    "# Here's a demo of the above ONNX model with ORT Web: https://jsbin.com/nupehazaju/edit?html,output  <-- seems to work, but this model doesn't have the projection head that squashes 768 vec to 512 elements (so can be compared to text embeddings of same length)\n",
    "# !pip install onnxruntime\n",
    "# !pip install onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "quantize_dynamic(\"clip-image-vit-32.onnx\", \"clip-image-vit-32-uint8.onnx\", weight_type=QuantType.QUInt8, extra_options={\"MatMulConstBOnly\":False}) # I added the MatMulConstBOnly as a guess due to warnings that it outputs without it\n",
    "\n",
    "# The code below is for converting to tflite, tfjs and tf saved model:\n",
    "\"\"\"\n",
    "!pip install git+https://github.com/onnx/onnx-tensorflow.git\n",
    "!onnx-tf convert -i clip-image-vit-32.onnx -o clip-image-vit-32-tf\n",
    "!onnx-tf convert -i clip-text-vit-32.onnx -o clip-text-vit-32-tf\n",
    "!pip install tensorflowjs\n",
    "!tensorflowjs_converter --input_format tf_saved_model ./clip-image-vit-32-tf ./clip-image-vit-32-tfjs\n",
    "!tensorflowjs_converter --input_format tf_saved_model ./clip-text-vit-32-tf ./clip-text-vit-32-tfjs\n",
    "\"\"\"\n",
    "     \n",
    "import tensorflow as tf\n",
    "# image encoder:\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"./clip-image-vit-32-tf\")\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS] # This line is needed because: https://github.com/tensorflow/tfjs/issues/5844\n",
    "tflite_model = converter.convert()\n",
    "with open('clip-image-vit-32.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)\n",
    "\n",
    "# text encoder:\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"./clip-text-vit-32-tf\")\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS] # This line is needed because: https://github.com/tensorflow/tfjs/issues/5844\n",
    "tflite_model = converter.convert()\n",
    "with open('clip-text-vit-32.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov77",
   "language": "python",
   "name": "yolov7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
