{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d814256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import skimage\n",
    "import random\n",
    "from PIL import Image\n",
    "# !pip install onnxruntime\n",
    "# !pip install onnx\n",
    "# !pip install git+https://github.com/onnx/onnx-tensorflow.git\n",
    "# !pip install tensorflowjs\n",
    "# !git clone https://github.com/apple/ml-mobileclip.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16aafe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--pretrained_path',type=str, default='./ml-mobileclip/checkpoints')\n",
    "    parser.add_argument('--model', default='mobileclip_s0', type=str)\n",
    "    return parser\n",
    "parser = get_args_parser()\n",
    "args = parser.parse_args(args=[])\n",
    "args.pretrained_path = os.path.join(args.pretrained_path, args.model) + '.pt'\n",
    "print('args.pretrained_path:', args.pretrained_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a4d1db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, preprocess = mobileclip.create_model_and_transforms(args, args.model, pretrained=args.pretrained_path, reparameterize=False)\n",
    "tokenizer = mobileclip.get_tokenizer(args.model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b967232",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_to_caption = [\n",
    "                        \"This is an example of a real face\",\n",
    "                        \"This is a bonafide face\",\n",
    "                        \"This is a real face\",\n",
    "                        \"This is how a real face looks like\",\n",
    "                        \"a photo of a real face\",\n",
    "                        \"This is not a spoof face\",\n",
    "                        \"This is an example of a spoof face\",\n",
    "                        \"This is an example of an attack face\",\n",
    "                        \"This is not a real face\",\n",
    "                        \"This is how a spoof face looks like\",\n",
    "                        \"a photo of a spoof face\",\n",
    "                        \"a printout shown to be a spoof face\",\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c61c6ba-4e89-4130-b4ca-e2f6a395d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = {\n",
    "    \"astronaut\": \"a portrait of an astronaut with the American flag\",\n",
    "}\n",
    "original_images = []\n",
    "images = []\n",
    "texts = []\n",
    "for filename in [filename for filename in os.listdir(skimage.data_dir) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]:\n",
    "    name = os.path.splitext(filename)[0]\n",
    "    if name not in descriptions:\n",
    "        continue\n",
    "    image = Image.open(os.path.join(skimage.data_dir, filename)).convert(\"RGB\")\n",
    "    original_images.append(image)\n",
    "    images.append(preprocess(image))\n",
    "    caption = random.choice(label_to_caption)\n",
    "    text = tokenizer(caption, return_tensors=\"pt\").squeeze()\n",
    "    texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f46733e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()\n",
    "model.eval()\n",
    "image_features = model.image_encoder(images[0].unsqueeze(0).cuda())\n",
    "text_features = model.text_encoder(texts[0].unsqueeze(0).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce81da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = './tflite_tmp'\n",
    "os.makedirs(output, exist_ok=True)\n",
    "# Text encoder part\n",
    "torch.onnx.export(model.text_encoder, texts[0].unsqueeze(0).cuda(), os.path.join(output,\"mobileclip-text-vit-32_v2.onnx\"), verbose=False, opset_version=12, input_names=['images'],\n",
    "                    output_names=['output'],\n",
    "                    dynamic_axes=None)\n",
    "# Image encoder part\n",
    "torch.onnx.export(model.image_encoder, images[0].unsqueeze(0).cuda(), os.path.join(output,\"mobileclip-image-vit-32_v2.onnx\"), verbose=False, opset_version=12, input_names=['images'],\n",
    "                    output_names=['output'],\n",
    "                    dynamic_axes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b78bb1c9-6f32-49db-96c7-ae4c3a333eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "# Attempt at quantizing model to uint8 (doesn't seem to work? no errors, but onnx file is same size)\n",
    "# Reference: https://github.com/minimaxir/imgbeddings/blob/36fb4d7ac6b82694d109cef6f887d4cb9c49da0f/imgbeddings/models.py#L94\n",
    "# Here's the model the above code generates: https://huggingface.co/minimaxir/imgbeddings/blob/main/patch32_v1.onnx\n",
    "# Here's a demo of the above ONNX model with ORT Web: https://jsbin.com/nupehazaju/edit?html,output  <-- seems to work, but this model doesn't have the projection head that squashes 768 vec to 512 elements (so can be compared to text embeddings of same length)\n",
    "\n",
    "# Transform float32 to utin8 of ONNX\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "quantize_dynamic(os.path.join(output,\"mobileclip-image-vit-32_v2.onnx\"), os.path.join(output,\"mogileclip-image-vit-32-uint8_v2.onnx\"), weight_type=QuantType.QUInt8, extra_options={\"MatMulConstBOnly\":False}) # I added the MatMulConstBOnly as a guess due to warnings that it outputs without it\n",
    "quantize_dynamic(os.path.join(output,\"mobileclip-text-vit-32_v2.onnx\"), os.path.join(output,\"mogileclip-text-vit-32-uint8_v2.onnx\"), weight_type=QuantType.QUInt8, extra_options={\"MatMulConstBOnly\":False}) # I added the MatMulConstBOnly as a guess due to warnings that it outputs without it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcb89680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-06 04:24:11.634695: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/minha.kim/miniconda3/envs/clipenv/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2024-08-06 04:24:13,342 - onnx-tf - INFO - Start converting onnx pb to tf saved model\n",
      "2024-08-06 04:24:13.944396: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-08-06 04:24:30,529 - onnx-tf - INFO - Converting completes successfully.\n",
      "INFO:onnx-tf:Converting completes successfully.\n"
     ]
    }
   ],
   "source": [
    "!onnx-tf convert -i tflite_tmp/mobileclip-image-vit-32_v2.onnx -o tflite_tmp/mobileclip-image-vit-32-tf_v2\n",
    "!onnx-tf convert -i tflite_tmp/mobileclip-text-vit-32_v2.onnx -o tflite_tmp/mobileclip-text-vit-32-tf_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bf1150",
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n",
    "import tensorflow as tf\n",
    "# image encoder:\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(output,\"mobileclip-image-vit-32-tf_v2\"))\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS] # This line is needed because: https://github.com/tensorflow/tfjs/issues/5844\n",
    "tflite_model = converter.convert()\n",
    "with open(os.path.join(output,'mobileclip-image-vit-32_v2.tflite'), 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7532f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text encoder:\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(os.path.join(output,\"mobileclip-text-vit-32-tf_v2\"))\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS] # This line is needed because: https://github.com/tensorflow/tfjs/issues/5844\n",
    "tflite_model = converter.convert()\n",
    "with open(os.path.join(output,'mobileclip-text-vit-32_v2.tflite'), 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
