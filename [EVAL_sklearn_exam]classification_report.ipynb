{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f40299ff-3bdd-47f0-b5a8-9ce1713e9221",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import copy\n",
    "import time\n",
    "from torch.cuda.amp import GradScaler\n",
    "from EarlyStopping import EarlyStopping\n",
    "from Common_Function_ import *\n",
    "from torch.autograd import Variable\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "GPU = '0,1,2,3'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPU\n",
    "device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# set_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f152a869-ea08-471d-bd21-86a17a19f306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "calculate = False\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "VALID_RATIO = 0.3\n",
    "N_IMAGES = 100\n",
    "START_LR = 1e-5\n",
    "END_LR = 10\n",
    "NUM_ITER = 100\n",
    "PATIENCE_EARLYSTOP=10\n",
    "\n",
    "pretrained_size = 224\n",
    "pretrained_means = [0.4489, 0.3352, 0.3106]#[0.485, 0.456, 0.406]\n",
    "pretrained_stds= [0.2380, 0.1965, 0.1962]#[0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c929cd0f-cfbe-401c-b87d-f5ec812572c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustumDataset(Dataset):\n",
    "    def __init__(self, data, target, data_2=None, target_2=None,transform=None):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.data_video = data_2    \n",
    "        self.target_video = target_2\n",
    "        self.transform = transform\n",
    "        \n",
    "        if self.data_video:\n",
    "            self.len_data2 = len(self.data_video)\n",
    "        print(self.len_data2)\n",
    "        print(len(self.data_video))\n",
    "        print(len(self.data))\n",
    "        \n",
    "        assert(self.len_data2 == len(self.target) == len(self.target_video) == len(self.data) == len(self.data_video))\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        path = self.data[idx]\n",
    "        img = Image.open(path)\n",
    "        img = img.convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        if self.data_video:\n",
    "            path_video = self.data[idx]\n",
    "            img_video = Image.open(path_video)\n",
    "            img_video = img_video.convert('RGB')\n",
    "            if self.transform:\n",
    "                img_video = self.transform(img_video)\n",
    "            return img, self.target[idx], img_video, self.target_video[idx]\n",
    "        return img, self.target[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "72b44348-0a00-401f-bbb2-ba966c029f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "                           transforms.Resize((pretrained_size,pretrained_size)),\n",
    "                           transforms.RandomHorizontalFlip(0.5),\n",
    "                           # transforms.RandomCrop(pretrained_size, padding = 10),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean = pretrained_means,\n",
    "                                                std = pretrained_stds)\n",
    "                       ])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "                           transforms.Resize((pretrained_size,pretrained_size)),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean = pretrained_means, \n",
    "                                                std = pretrained_stds)\n",
    "                       ])\n",
    "\n",
    "train_dir = [\"/media/data1/mhkim/FAKEVV_hasam/train/SPECTOGRAMS/real_A_fake_B\",\n",
    "             \"/media/data1/mhkim/FAKEVV_hasam/train/FRAMES/real_A_fake_MergedC\"]\n",
    "test_dir =  [\"/media/data1/mhkim/FAKEVV_hasam/test/SPECTOGRAMS/real_A_fake_others\",\n",
    "             \"/media/data1/mhkim/FAKEVV_hasam/test/FRAMES/real_A_fake_others\"]\n",
    "\n",
    "list_train = [datasets.ImageFolder(root = train_dir[0],transform = None),\n",
    "             datasets.ImageFolder(root = train_dir[1],transform = None)]\n",
    "list_test = [datasets.ImageFolder(root = test_dir[0],transform = None),\n",
    "            datasets.ImageFolder(root = test_dir[1],transform = None)]\n",
    "\n",
    "\n",
    "list_glob_trainpath_video = [list_train[0].samples[i][0] for i in range(len(list_train[0].samples))]\n",
    "list_glob_trainpath = [list_train[1].samples[i][0] for i in range(len(list_train[1].samples))]\n",
    "list_targets_trainpath_video = [list_train[0].targets[i] for i in range(len(list_train[0].targets))]\n",
    "list_targets_trainpath = [list_train[1].targets[i] for i in range(len(list_train[1].targets))]\n",
    "\n",
    "list_glob_testpath_video = [list_test[0].samples[i][0] for i in range(len(list_test[0].samples))]\n",
    "list_glob_testpath = [list_test[1].samples[i][0] for i in range(len(list_test[1].samples))]\n",
    "list_targets_testpath_video = [list_test[0].targets[i] for i in range(len(list_test[0].targets))]\n",
    "list_targets_testpath = [list_test[1].targets[i] for i in range(len(list_test[1].targets))]\n",
    "# print(list_targets_testpath[3402])\n",
    "# print(list_targets_testpath_video.count(0))\n",
    "# print(len(list_targets_testpath_video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f1b1385-9413-4665-b0c6-4ab6a03e6a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head is 27\n",
      "True\n",
      "4294\n",
      "4294\n",
      "4294\n",
      "head is 176\n",
      "True\n",
      "127356\n",
      "127356\n",
      "127356\n"
     ]
    }
   ],
   "source": [
    "###################TARGET##################\n",
    "cnt_target_video = list_targets_testpath_video.count(0)\n",
    "cnt_target = list_targets_testpath.count(0)\n",
    "temp_list = list_targets_testpath_video[:cnt_target_video]\n",
    "temp_list_input = list_glob_testpath_video[:cnt_target_video]\n",
    "head = cnt_target//cnt_target_video\n",
    "print('head is',head)\n",
    "list_0 , list_1 = {'input':[],'target':[]},{'input':[],'target':[]}\n",
    "for c in range(head):\n",
    "    list_0['target']+=temp_list\n",
    "    list_0['input']+=temp_list_input\n",
    "list_0['target']+=temp_list[:(cnt_target-len(list_0['target']))]\n",
    "list_1['target']+=list_targets_testpath[cnt_target:]\n",
    "list_0['input']+=temp_list_input[:(cnt_target-len(list_0['input']))]\n",
    "list_1['input']+=list_glob_testpath[cnt_target:]\n",
    "\n",
    "\n",
    "list_test_target_videos = list_0['target'] + list_1['target']\n",
    "list_test_input_videos = list_0['input'] + list_1['input']\n",
    "# print(len(list_test_target_videos))\n",
    "print(list_test_target_videos==list_targets_testpath)\n",
    "\n",
    "assert((len(list_test_target_videos))==(len(list_targets_testpath))==len(list_test_input_videos))\n",
    "\n",
    "print(len(list_test_target_videos))\n",
    "print(len(list_test_input_videos))\n",
    "print(len(list_targets_testpath))\n",
    "\n",
    "##################TRAIN####################\n",
    "cnt_train_video = list_targets_trainpath_video.count(0)\n",
    "cnt_train = list_targets_trainpath.count(0)\n",
    "temp_list = list_targets_trainpath_video[:cnt_train_video]\n",
    "temp_list_input = list_glob_trainpath_video[:cnt_train_video]\n",
    "head = cnt_train//cnt_train_video\n",
    "print('head is',head)\n",
    "list_0 , list_1 = {'input':[],'target':[]},{'input':[],'target':[]}\n",
    "for c in range(head):\n",
    "    list_0['target']+=temp_list\n",
    "    list_0['input']+=temp_list_input\n",
    "list_0['target']+=temp_list[:(cnt_train-len(list_0['target']))]\n",
    "list_1['target']+=list_targets_trainpath[cnt_train:]\n",
    "list_0['input']+=temp_list_input[:(cnt_train-len(list_0['input']))]\n",
    "list_1['input']+=list_glob_trainpath[cnt_train:]\n",
    "\n",
    "list_train_target_videos = list_0['target'] + list_1['target']\n",
    "list_train_input_videos = list_0['input'] + list_1['input']\n",
    "print(list_train_target_videos==list_targets_trainpath)\n",
    "assert((len(list_train_target_videos))==(len(list_targets_trainpath))==len(list_train_input_videos))\n",
    "\n",
    "print(len(list_train_target_videos))\n",
    "print(len(list_targets_trainpath))\n",
    "print(len(list_train_input_videos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ffcb93d-0713-47cc-a0c6-979b4658945c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127356\n",
      "127356\n",
      "127356\n",
      "4294\n",
      "4294\n",
      "4294\n",
      "139 139\n",
      "4294 4294\n",
      "Number of training examples: 89150\n",
      "Number of validation examples: 38206\n",
      "Number of testing examples: 4294\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.utils.data as data\n",
    "#image, video 한꺼번에 저장\n",
    "# list_glob_trainpath = [list_train[0].samples[i][0] for i in range(len(list_train[0].samples))]\n",
    "# list_glob_trainpath += [list_train[1].samples[i][0] for i in range(len(list_train[1].samples))]\n",
    "# list_targets_trainpath = [list_train[0].targets[i] for i in range(len(list_train[0].targets))]\n",
    "# list_targets_trainpath += [list_train[1].targets[i] for i in range(len(list_train[1].targets))]\n",
    "# list_glob_testpath = [list_test[0].samples[i][0] for i in range(len(list_test[0].samples))]\n",
    "# list_glob_testpath += [list_test[1].samples[i][0] for i in range(len(list_test[1].samples))]\n",
    "# list_targets_testpath = [list_test[0].targets[i] for i in range(len(list_test[0].targets))]\n",
    "# list_targets_testpath += [list_test[1].targets[i] for i in range(len(list_test[1].targets))]\n",
    "\n",
    "###########################\n",
    "# train_data_video = CustumDataset(list_glob_trainpath_video, list_targets_trainpath_video, test_transforms)\n",
    "\n",
    "# n_valid_examples_video = int(len(train_data_video) * 0.2)\n",
    "# n_train_examples_video = len(train_data_video) - n_valid_examples_video\n",
    "# test_data_video = CustumDataset(list_glob_testpath_video, list_targets_testpath_video, test_transforms)\n",
    "\n",
    "train_data = CustumDataset(list_glob_trainpath, list_targets_trainpath, list_train_input_videos, list_train_target_videos,test_transforms)\n",
    "n_valid_examples = int(len(train_data) * VALID_RATIO)#기존 test data자체가 너무 적어서 train기준으로 비율조정\n",
    "n_train_examples = len(train_data) - n_valid_examples\n",
    "\n",
    "train_data, valid_data = data.random_split(train_data, [n_train_examples, n_valid_examples])\n",
    "# train_data_video, valid_data_video = data.random_split(train_data_video, [n_train_examples_video, n_valid_examples_video])\n",
    "\n",
    "train_data.transform = train_transforms; train_data.dataset.transform = train_transforms\n",
    "valid_data.transform = test_transforms; valid_data.dataset.transform = test_transforms\n",
    "\n",
    "test_data = CustumDataset(list_glob_testpath, list_targets_testpath, list_test_input_videos, list_test_target_videos, test_transforms)\n",
    "print(len(list_glob_testpath_video), len(list_targets_testpath_video))\n",
    "print(len(list_test_input_videos), len(list_test_target_videos))\n",
    "from PIL import Image\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n",
    "\n",
    "train_iterator = data.DataLoader(train_data, \n",
    "                                 shuffle = True, \n",
    "                                 batch_size = BATCH_SIZE)\n",
    "valid_iterator = data.DataLoader(valid_data, \n",
    "                                 shuffle = True, \n",
    "                                 batch_size = BATCH_SIZE)\n",
    "test_iterator = data.DataLoader(test_data, \n",
    "                                shuffle = True, \n",
    "                                batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed5a80b-fab0-40d7-a3c7-ef78c4f4116e",
   "metadata": {},
   "source": [
    "### predict list중 max인 친구는 1로 바꾸고 모두 0으로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2d1a8c97-6be2-466e-a9b9-61a8347833eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " tensor([[ 0.1868,  1.2265,  1.0181,  0.6943],\n",
      "        [ 0.1075, -0.3540,  0.1766, -0.4940],\n",
      "        [ 3.0013,  1.8697, -0.0673, -1.3875],\n",
      "        [-0.0560, -0.6007, -0.0410, -0.6681],\n",
      "        [-0.2963,  0.1108,  0.2250, -0.5483],\n",
      "        [-0.1541, -1.1390,  0.4984, -0.4016],\n",
      "        [-0.3887, -0.1821,  0.4817,  0.4268],\n",
      "        [ 0.8756,  2.0755,  0.2577,  0.8941]]) \n",
      "\n",
      " tensor([1, 2, 0, 2, 2, 2, 2, 1]) \n",
      "\n",
      " tensor([[0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed (1414)\n",
    "t = torch.randn (8, 4)\n",
    "a = t.argmax (1)\n",
    "m = torch.zeros (t.shape).scatter (1, a.unsqueeze (1), 1.0)\n",
    "print ('\\n', t, '\\n\\n', a, '\\n\\n', m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946b4cfd-6852-4e61-aa3c-c67305f99577",
   "metadata": {},
   "source": [
    "### EXAMPLE on Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78e6757d-85e4-47b9-92f8-9a4a398a5bb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ecls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_94335/3879418162.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mecls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ecls' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "ecls.eval()\n",
    "for i, data in enumerate(test_iterator):\n",
    "    with torch.no_grad():\n",
    "        in_1 = in_1.to(device)\n",
    "        in_2 = in_2.to(device)\n",
    "        print(f'{in_1.size()}, {in_2.size()}')\n",
    "        y_pred = ecls(in_1,in_2).cpu().detach()\n",
    "        \n",
    "        in_1 = torch.tensor(data[0].detach().cpu())\n",
    "        in_2 = torch.tensor(data[2].detach().cpu())\n",
    "        integer_encoded = label_encoder.fit_transform(data[1].detach().cpu())\n",
    "        integer_encoded_2 = label_encoder.fit_transform(data[3].detach().cpu())\n",
    "        integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "        integer_encoded_2 = integer_encoded_2.reshape(len(integer_encoded_2), 1)\n",
    "        \n",
    "        onehot_encoded = enc.fit_transform(integer_encoded)\n",
    "        onehot_encoded_2 = enc.fit_transform(integer_encoded_2)\n",
    "        onehot_encoded =onehot_encoded.astype(np.int8) \n",
    "        onehot_encoded_2 =onehot_encoded_2.astype(np.int8) \n",
    "        \n",
    "        y_true = torch.tensor(onehot_encoded + onehot_encoded_2)\n",
    "        y_true_argmax = y_true.argmax(1)\n",
    "        y_true = np.array(torch.zeros(y_true.shape).scatter(1, y_true_argmax.unsqueeze(1),1),dtype=np.int8)\n",
    "        print(y_true)\n",
    "\n",
    "        a = y_pred.argmax (1)\n",
    "        y_pred = torch.zeros (y_pred.shape).scatter (1, a.unsqueeze(1), 1)\n",
    "        result = classification_report(y_true, y_pred, labels=None, target_names=None, sample_weight=None, digits=5, output_dict=False, zero_division='warn')\n",
    "        print(result)\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "569557cf-c5f9-43c4-9069-e1ee2d1f4701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.72727   0.88889   0.80000        27\n",
      "           1    0.90323   0.75676   0.82353        37\n",
      "\n",
      "   micro avg    0.81250   0.81250   0.81250        64\n",
      "   macro avg    0.81525   0.82282   0.81176        64\n",
      "weighted avg    0.82900   0.81250   0.81360        64\n",
      " samples avg    0.81250   0.81250   0.81250        64\n",
      "\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(result)\n",
    "print(type(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5dc0fc-b5b3-4536-8445-2868fde95c48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f28e755-ecb9-4035-88b2-162712c5e364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186af1a7-316e-4537-9690-47812169f884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2d210e3e-beb3-46ca-ba07-029987a1480a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train/val/test loader : (1393, 597, 68)\n"
     ]
    }
   ],
   "source": [
    "from models.MesoNet4_forEnsemble import MesoInception4 as MesoInception4\n",
    "print(f'number of train/val/test loader : {len(train_iterator), len(valid_iterator), len(test_iterator)}')\n",
    "models = [MesoInception4(num_classes=2), MesoInception4(num_classes=2)]\n",
    "MODELS_NAME = 'MesoInception4'\n",
    "#checkpoinsts for model loaders :  [VIDEO(A&B), FRAME(A&C)]\n",
    "list_checkpoint = [torch.load(f'/home/mhkim/DFVV/PRETRAINING/{MODELS_NAME}_realA_fakeB.pt')['state_dict'],\n",
    "                   torch.load(f'/home/mhkim/DFVV/PRETRAINING/{MODELS_NAME}_realA_fakeC.pt')['state_dict']]\n",
    "models[0].load_state_dict(list_checkpoint[0])\n",
    "models[1].load_state_dict(list_checkpoint[1])\n",
    "OUTPUT_DIM = 2\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "scaler = GradScaler()\n",
    "early_stopping = EarlyStopping(patience=PATIENCE_EARLYSTOP, verbose=True)\n",
    "\n",
    "# from torchsummary import summary\n",
    "# summary(models[0].cuda(), (3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6b4598c0-245d-420f-aecd-91bca5721610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble(nn.Module):\n",
    "    def __init__(self,models,device,training=True):\n",
    "        super().__init__()\n",
    "        self.model1 = models[0] #FOR VIDEO\n",
    "        self.model2 = models[1] #FOR FRAME IMAGE\n",
    "        self.fc1 = nn.Linear(16 ,224)#.to(device)\n",
    "        self.fc2 = nn.Linear(16, 224)#.to(device)\n",
    "        self.fc = nn.Linear(224,2)\n",
    "#         selr.drop \n",
    "    def forward(self, frame, video):\n",
    "        feat1 = self.model1(video)\n",
    "        feat2 = self.model2(frame)\n",
    "        inp1 = feat1.view(feat1.size(0), -1)\n",
    "        inp2 = feat2.view(feat2.size(0), -1)\n",
    "        out1 = self.fc1(inp1)\n",
    "        out2 = self.fc2(inp2)\n",
    "        out = out1 + out2\n",
    "        out = out/2\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "ecls = Ensemble(models,device)\n",
    "ecls = nn.DataParallel(ecls)   # 4개의 GPU를 이용할 경우\n",
    "ecls = ecls.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79ab840b-f27c-43ea-9491-b1b8daad7505",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(ecls.parameters(), lr = START_LR)\n",
    "scaler = GradScaler()\n",
    "STEPS_PER_EPOCH = len(train_iterator)\n",
    "TOTAL_STEPS = EPOCHS * STEPS_PER_EPOCH\n",
    "best_valid_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef341301-d61b-4128-af3d-91ab755592e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def train_voting(model, iterator, optimizer, criterion, scaler, device):\n",
    "    epoch_loss = 0\n",
    "    output=[]\n",
    "    model.train()\n",
    "    correct, total = 0,0\n",
    "    \n",
    "    for (x, y, x_v, y_v) in tqdm(iterator):\n",
    "        with autocast(enabled=True):\n",
    "            x, y, x_v, y_v = x.to(device), y.to(device), x_v.to(device), y_v.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            final_output = model(x, x_v)\n",
    "            loss = torch.FloatTensor([0.]).to(device)\n",
    "            loss += criterion(final_output,y)\n",
    "            \n",
    "            scaler.scale(loss).backward(retain_graph=True)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            _, predicted = torch.max(final_output.data, 1)\n",
    "            correct += predicted.eq(y.data).cpu().sum()\n",
    "            total += y.size(0)\n",
    "        \n",
    "            epoch_loss += loss.item()\n",
    "    print(\"Accuracy = {}\".format(100. * correct / total))\n",
    "\n",
    "    epoch_loss /= len(iterator)\n",
    "    epoch_acc = correct / total \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def evaluate_voting(model, iterator, criterion, device):\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    correct, total = 0,0\n",
    "    with torch.no_grad():\n",
    "        for (x, y, x_v, y_v) in tqdm(iterator):\n",
    "            x, y, x_v, y_v = x.to(device), y.to(device), x_v.to(device), y_v.to(device)\n",
    "            final_output = model(x, x_v)\n",
    "            loss = torch.FloatTensor([0.]).to(device)\n",
    "            loss += criterion(final_output,y)\n",
    "            \n",
    "            _, predicted = torch.max(final_output.data, 1)\n",
    "            correct += predicted.eq(y.data).cpu().sum()\n",
    "            total += y.size(0)\n",
    "            epoch_loss += loss.item()\n",
    "    epoch_loss /= len(iterator)\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "441be5eb-4b7a-4511-b499-36387d88dc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 71.549072265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 11m 14s\n",
      "\tTrain Loss: 0.520 | Train Acc @1:  71.55% | \n",
      "\tValid Loss: 0.440 | Valid Acc @1:  79.09% | \n",
      "Validation loss decreased (inf --> 0.439572).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 83.68704223632812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Epoch Time: 10m 59s\n",
      "\tTrain Loss: 0.367 | Train Acc @1:  83.69% | \n",
      "\tValid Loss: 0.624 | Valid Acc @1:  67.63% | \n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 85.61077117919922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 597/597 [02:43<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Epoch Time: 10m 51s\n",
      "\tTrain Loss: 0.333 | Train Acc @1:  85.61% | \n",
      "\tValid Loss: 0.718 | Valid Acc @1:  64.37% | \n",
      "EarlyStopping counter: 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 1393/1393 [07:25<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 86.78182983398438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 597/597 [02:42<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Epoch Time: 10m 7s\n",
      "\tTrain Loss: 0.310 | Train Acc @1:  86.78% | \n",
      "\tValid Loss: 0.776 | Valid Acc @1:  62.98% | \n",
      "EarlyStopping counter: 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 1393/1393 [07:24<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 87.8328628540039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 597/597 [02:42<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Epoch Time: 10m 7s\n",
      "\tTrain Loss: 0.290 | Train Acc @1:  87.83% | \n",
      "\tValid Loss: 0.871 | Valid Acc @1:  61.01% | \n",
      "EarlyStopping counter: 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 1393/1393 [07:24<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 88.79753112792969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 597/597 [02:46<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Epoch Time: 10m 10s\n",
      "\tTrain Loss: 0.269 | Train Acc @1:  88.80% | \n",
      "\tValid Loss: 0.929 | Valid Acc @1:  59.72% | \n",
      "EarlyStopping counter: 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 1393/1393 [07:29<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 89.69377136230469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 597/597 [02:45<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Epoch Time: 10m 14s\n",
      "\tTrain Loss: 0.253 | Train Acc @1:  89.69% | \n",
      "\tValid Loss: 0.784 | Valid Acc @1:  63.57% | \n",
      "EarlyStopping counter: 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 1393/1393 [07:24<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.2669677734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 597/597 [02:41<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Epoch Time: 10m 6s\n",
      "\tTrain Loss: 0.240 | Train Acc @1:  90.27% | \n",
      "\tValid Loss: 0.905 | Valid Acc @1:  60.49% | \n",
      "EarlyStopping counter: 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 1393/1393 [07:30<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.77397918701172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 597/597 [02:44<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Epoch Time: 10m 14s\n",
      "\tTrain Loss: 0.228 | Train Acc @1:  90.77% | \n",
      "\tValid Loss: 0.921 | Valid Acc @1:  61.45% | \n",
      "EarlyStopping counter: 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 1393/1393 [07:06<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 91.19910430908203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 597/597 [02:24<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Epoch Time: 9m 31s\n",
      "\tTrain Loss: 0.218 | Train Acc @1:  91.20% | \n",
      "\tValid Loss: 1.043 | Valid Acc @1:  59.28% | \n",
      "EarlyStopping counter: 9 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 1393/1393 [07:06<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 91.84745025634766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████| 597/597 [02:42<00:00,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Epoch Time: 9m 49s\n",
      "\tTrain Loss: 0.206 | Train Acc @1:  91.85% | \n",
      "\tValid Loss: 0.915 | Valid Acc @1:  61.17% | \n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.monotonic()\n",
    "    train_loss, train_acc = train_voting(ecls, train_iterator, optimizer, criterion, scaler, device)\n",
    "    valid_loss, valid_acc = evaluate_voting(ecls, valid_iterator, criterion, device)\n",
    "        \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save({'state_dict':ecls.state_dict(),\n",
    "                   'best_acc':valid_acc,\n",
    "                   'val_loss':valid_loss,\n",
    "                   'epoch':epoch,\n",
    "                   'lr':START_LR,\n",
    "                   'best_acc':valid_acc,\n",
    "                   }, f'[SFVoting,{MODELS_NAME}]with_pretrained.pt')\n",
    "\n",
    "    end_time = time.monotonic()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc @1: {train_acc*100:6.2f}% | ')\n",
    "    print(f'\\tValid Loss: {valid_loss:.3f} | Valid Acc @1: {valid_acc*100:6.2f}% | ')\n",
    "    \n",
    "    if early_stopping:\n",
    "        early_stopping(valid_loss, ecls)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adea9d4-4f17-45b5-bd2c-bec64193beba",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "trn_feat_loader, val_feat_loader, test_feat_loader = Get_loader_feats(models,train_iterator,valid_iterator,test_iterator,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee8e20b-a608-4b26-8957-6d4f768fe3f5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "from sklearn import model_selection\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "labels = ['model1', 'model2']\n",
    "device= 'cuda'\n",
    "ecls.train()\n",
    "for i, (x,y) in enumerate(tqdm(train_iterator)):\n",
    "    print(y)\n",
    "    \n",
    "    with autocast(enabled=True):\n",
    "        x = x.to(device); y= y.to(device)\n",
    "        output = []\n",
    "        optimizer.zero_grad()\n",
    "        for clf, label in zip([models[0], models[1]], labels):\n",
    "            output.append(clf.module.features(x))\n",
    "        print(type(output))\n",
    "        final_output = ecls(output[0],output[1])\n",
    "        print(final_output)\n",
    "        loss = torch.FloatTensor([0.]).to(device)\n",
    "        loss += criterion(final_output,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b661c116-30ed-4971-b98d-aedfd14f738c",
   "metadata": {
    "tags": []
   },
   "source": [
    "import copy\n",
    "def train_voting(model, iterator, optimizer, criterion, scaler, device):\n",
    "    epoch_loss = 0\n",
    "    output=[]\n",
    "    model.train()\n",
    "    correct, total = 0,0\n",
    "    \n",
    "    for (x, y) in tqdm(iterator):\n",
    "        with autocast(enabled=True):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "#             _inp1, _inp2 = x, copy.deepcopy(x)\n",
    "            optimizer.zero_grad()\n",
    "            final_output = model(x)\n",
    "            loss = torch.FloatTensor([0.]).to(device)\n",
    "            loss += criterion(final_output,y)\n",
    "            total += y.size(0)\n",
    "            \n",
    "            acc_1, acc_5 = calculate_topk_accuracy(final_output, y)\n",
    "            scaler.scale(loss).backward(retain_graph=True)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            _, predicted = torch.max(final_output.data, 1)\n",
    "            correct += float(predicted.eq(y.data).cpu().sum())\n",
    "            total += float(y.size(0))\n",
    "        \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    print(\"Accuracy = {}\".format(100. * correct / total))\n",
    "\n",
    "    epoch_loss /= len(iterator)\n",
    "    epoch_acc = correct / total * 100\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def evaluate_voting(model, iterator, criterion, device):\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    correct, total = 0,0\n",
    "    with torch.no_grad():\n",
    "        for (x, y) in tqdm(iterator):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            final_output = model(x)\n",
    "            loss = torch.FloatTensor([0.]).to(device)\n",
    "            loss += criterion(final_output,y)\n",
    "            \n",
    "            _, predicted = torch.max(final_output.data, 1)\n",
    "            correct += float(predicted.eq(y.data).cpu().sum())\n",
    "            total += float(y.size(0))\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(iterator)\n",
    "    epoch_acc = correct / total * 100\n",
    "\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16c4d95-cf80-4334-954e-acbf5ea47a03",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "def train_voting(model, iterator, optimizer, criterion, scaler, device):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc_1 = 0\n",
    "    epoch_acc_5 = 0\n",
    "    labels = ['model1', 'model2']\n",
    "    output=[]\n",
    "    model.train()\n",
    "    tot = 0\n",
    "    correct = 0\n",
    "    for (x, y) in tqdm(iterator):\n",
    "        with autocast(enabled=True):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            for clf, label in zip([models[0], models[1]], labels):\n",
    "                output.append(clf.module.features(Variable(x)))\n",
    "            final_output = ecls(output[0],output[1])\n",
    "            loss = torch.FloatTensor([0.]).to(device)\n",
    "            loss += criterion(final_output,y)\n",
    "#             print(loss)\n",
    "            tot += y.size(0)\n",
    "            \n",
    "            acc_1, acc_5 = calculate_topk_accuracy(final_output, y)\n",
    "            scaler.scale(loss).backward(retain_graph=True)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            _, predicted = final_output.max(1)\n",
    "            correct += predicted.eq(y).sum().item()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc_1 += acc_1.item()\n",
    "    print(\"Accuracy = {}\".format(100. * correct / tot))\n",
    "\n",
    "    epoch_loss /= len(iterator)\n",
    "    epoch_acc_1 /= len(iterator)\n",
    "    epoch_acc_5 /= len(iterator)\n",
    "\n",
    "    return epoch_loss, epoch_acc_1, epoch_acc_5\n",
    "\n",
    "\n",
    "def evaluate_voting(model, iterator, criterion, device):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc_1 = 0\n",
    "    epoch_acc_5 = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (x, y) in tqdm(iterator):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            acc_1, acc_5 = calculate_topk_accuracy(y_pred, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc_1 += acc_1.item()\n",
    "            epoch_acc_5 += acc_5.item()\n",
    "\n",
    "    epoch_loss /= len(iterator)\n",
    "    epoch_acc_1 /= len(iterator)\n",
    "    epoch_acc_5 /= len(iterator)\n",
    "\n",
    "    return epoch_loss, epoch_acc_1, epoch_acc_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9779f379-82f5-468f-bc56-d181ad9becf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gogogo",
   "language": "python",
   "name": "mhmh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
